{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TriHSPAM\n",
    "### Experiments with ALS dataset\n",
    "with Strict Alignments Option"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 0. Preprocessing & Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "    # Function to convert float(nan) to np.nan\n",
    "def convert_nan(arr):\n",
    "    if isinstance(arr, float):\n",
    "        return np.nan\n",
    "    else:\n",
    "        return arr\n",
    "def read_transactions_csv(file_path):\n",
    "    visits = {}\n",
    "    with open(file_path, 'r') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        for row in csv_reader:\n",
    "            client_id = row['id']\n",
    "            if client_id not in visits:\n",
    "                visits[client_id] = []\n",
    "            visits[client_id].append(row)\n",
    "    \n",
    "    max_transactions = max(len(visits[client_id]) for client_id in visits)\n",
    "    num_clients = len(visits)\n",
    "    num_features = len(visits[next(iter(visits))][0]) - 1  # Excluding patientID\n",
    "    \n",
    "    # Create a 3D numpy array to store visits\n",
    "    transactions_array = np.empty((max_transactions, num_clients, num_features), dtype=object)\n",
    "    \n",
    "    for i, (client_id, client_transactions) in enumerate(visits.items()):\n",
    "        for j, transaction in enumerate(client_transactions):\n",
    "            transaction_details = [val if val is not None and val != \"\" else np.nan for key, val in transaction.items() if key != 'id']\n",
    "            transactions_array[j, i, :] = transaction_details\n",
    "        # If a patient has fewer transactions than the maximum, pad with np.nan\n",
    "        for j in range(len(client_transactions), max_transactions):\n",
    "            transactions_array[j, i, :] = np.nan\n",
    "    transactions_array = np.transpose(transactions_array, (2,1,0))\n",
    "    # transactions_array = np.vectorize(convert_nan)(transactions_array)\n",
    "\n",
    "    return transactions_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 1715, 20)\n"
     ]
    }
   ],
   "source": [
    "transactions_3d_array = read_transactions_csv(\"realData/ALS_data/ALS_snapshots_modified_subscores.csv\")\n",
    "print(transactions_3d_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Triclustering with TriHSPAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TriHSPAM import TriHSPAM\n",
    "triclustering = TriHSPAM(symb_features_idx=[0,5,6,13,14],\n",
    "                          num_features_idx=[1,2,3,4,7,8,9,10,11,12],\n",
    "                          min_I= 170,\n",
    "                          min_J=2, \n",
    "                          min_K=2,\n",
    "                          disc_method=\"eq_width\",\n",
    "                          n_bins=10,\n",
    "                          time_as_cols=True,\n",
    "                          time_relaxed=False,\n",
    "                          spm_algo='fournier08closed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">/Users/diogosoares/Documents/PhD Work/TriHSPAM/src/spmf_vd.jar\n",
      "=============  Algorithm - STATISTICS =============\n",
      " Total time ~ 495882 ms\n",
      " Frequent sequences count : 3185\n",
      "===================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<TriHSPAM.TriHSPAM at 0x10c8b3e50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triclustering.fit(transactions_3d_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trics = triclustering.triclusters_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2718"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1715, 15, 20)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions_3d_array.transpose((1,0,2)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Assessing Statistical Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sigStats import Tricluster, significance\n",
    "import copy\n",
    "\n",
    "lst_trics = []\n",
    "data_f = copy.deepcopy(transactions_3d_array.transpose((1,0,2)))\n",
    "Y_cat = [0,5,6,13,14,9,10,11,12]\n",
    "            \n",
    "for t_id, tric in enumerate(trics):\n",
    "    t = Tricluster(data_f, Y_cat, sorted(tric[0]), tric[1], tric[2])\n",
    "    lst_trics.append(t)\n",
    "\n",
    "p_values = significance(data_f, Y_cat, lst_trics, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sign_stats = []\n",
    "\n",
    "for t_i, p_val in enumerate(p_values):\n",
    "    if p_val < 0.001:\n",
    "        row = {'TricID':t_i, 'p-value':p_val}\n",
    "        sign_stats.append(row)\n",
    "\n",
    "df_stat = pd.DataFrame(sign_stats)\n",
    "df_stat.to_csv(\"signStats_als_relaxed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TricID</th>\n",
       "      <th>p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2464</th>\n",
       "      <td>2464</td>\n",
       "      <td>7.120236e-306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2053</th>\n",
       "      <td>2053</td>\n",
       "      <td>7.899012e-306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>1918</td>\n",
       "      <td>8.299525e-306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2548</th>\n",
       "      <td>2548</td>\n",
       "      <td>1.386221e-305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>2002</td>\n",
       "      <td>1.466324e-305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>941</td>\n",
       "      <td>1.655455e-305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>988</td>\n",
       "      <td>1.682156e-305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>388</td>\n",
       "      <td>1.726657e-305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>435</td>\n",
       "      <td>1.753358e-305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>2499</td>\n",
       "      <td>1.831236e-305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TricID        p-value\n",
       "2464    2464  7.120236e-306\n",
       "2053    2053  7.899012e-306\n",
       "1918    1918  8.299525e-306\n",
       "2548    2548  1.386221e-305\n",
       "2002    2002  1.466324e-305\n",
       "941      941  1.655455e-305\n",
       "988      988  1.682156e-305\n",
       "388      388  1.726657e-305\n",
       "435      435  1.753358e-305\n",
       "2499    2499  1.831236e-305"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stat.sort_values('p-value').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Patterns Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1396\t['slope', 'bulbar_subscore', 'respiratory_subscore']\t[0, 1]\n",
      "[[0.68, 12.0, 12.0], [0.68, 11.0, 12.0]]\n",
      "1361\t['height', 'slope']\t[0, 1]\n",
      "[[1.64, 0.67], [1.64, 0.67]]\n",
      "1343\t['height', 'slope', 'bulbar_subscore', 'respiratory_subscore']\t[0, 1]\n",
      "[[1.64, 0.67, 12.0, 12.0], [1.64, 0.67, 11.0, 12.0]]\n",
      "1093\t['slope', 'alsfrs_r_tot_score', 'bulbar_subscore', 'respiratory_subscore']\t[0, 1]\n",
      "[[0.53, 44.0, 12.0, 12.0], [0.53, 42.0, 11.0, 12.0]]\n",
      "1057\t['height', 'slope', 'alsfrs_r_tot_score', 'bulbar_subscore', 'respiratory_subscore']\t[0, 1]\n",
      "[[1.65, 0.53, 44.0, 12.0, 12.0], [1.65, 0.53, 42.0, 11.0, 12.0]]\n",
      "972\t['slope', 'Onset_form']\t[0, 1]\n",
      "[[0.68, 'onset_limbs'], [0.68, 'onset_limbs']]\n",
      "960\t['slope', 'bulbar_subscore', 'respiratory_subscore', 'Onset_form']\t[0, 1]\n",
      "[[0.68, 12.0, 12.0, 'onset_limbs'], [0.68, 12.0, 12.0, 'onset_limbs']]\n",
      "940\t['height', 'slope', 'Onset_form']\t[0, 1]\n",
      "[[1.65, 0.68, 'onset_limbs'], [1.65, 0.68, 'onset_limbs']]\n",
      "928\t['height', 'slope', 'bulbar_subscore', 'respiratory_subscore', 'Onset_form']\t[0, 1]\n",
      "[[1.65, 0.68, 12.0, 12.0, 'onset_limbs'], [1.65, 0.68, 12.0, 12.0, 'onset_limbs']]\n",
      "893\t['slope', 'bulbar_subscore', 'motor_subscore', 'respiratory_subscore']\t[0, 1]\n",
      "[[0.47, 12.0, 22.0, 12.0], [0.47, 11.0, 21.0, 12.0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diogosoares/Documents/PhD Work/TriHSPAM/src/visualize_patterns.py:26: RuntimeWarning: Mean of empty slice\n",
      "  vals[i] = round(np.nanmean(converted_column),2)\n",
      "/Users/diogosoares/Library/Python/3.8/lib/python/site-packages/numpy/lib/nanfunctions.py:1217: RuntimeWarning: All-NaN slice encountered\n",
      "  return function_base._ureduce(a, func=_nanmedian, keepdims=keepdims,\n"
     ]
    }
   ],
   "source": [
    "from visualize_patterns import compute_representative_patterns\n",
    "\n",
    "df = pd.read_csv(\"realData/ALS_data/ALS_snapshots_modified_subscores.csv\", sep=',')\n",
    "data_modes = [0,5,6,13,14]\n",
    "data_means = [1,2,3,4,7,8]\n",
    "data_medians = [9,10,11,12]\n",
    "\n",
    "for t_id in df_stat.sort_values('p-value').head(10)['TricID']:\n",
    "    tric_dims = trics[t_id]\n",
    "    print(len(tric_dims[0]), list(df.columns[1:][tric_dims[1]]), tric_dims[2], sep='\\t')\n",
    "    print(compute_representative_patterns(triclustering.get_tricluster(t_id), \n",
    "                                        mode_feats=[i for i in tric_dims[1] if i in data_modes],\n",
    "                                        mean_features=[i for i in tric_dims[1] if i in data_means],\n",
    "                                        median_features=[i for i in tric_dims[1] if i in data_medians]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
